{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = {\n",
    "    'IMDB-MULTI': 3,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_config = '''\n",
    "out_dir: {out_dir}\n",
    "metric_best: auto\n",
    "metric_agg: argmin\n",
    "device: 'cuda:0'\n",
    "wandb:\n",
    "  use: True\n",
    "  project: {wandb_proj_name}\n",
    "  entity: <WANDB_USERNAME>\n",
    "dataset:\n",
    "  format: PyG\n",
    "  name: {ds}\n",
    "  dir: {ds_download_dir}\n",
    "  onehot: True\n",
    "  target_name: None\n",
    "  task: graph\n",
    "  task_type: {task_type}\n",
    "  transductive: False\n",
    "  node_encoder: True\n",
    "  node_encoder_name: LinearNode+RWSE\n",
    "  node_encoder_num_types: 28\n",
    "  node_encoder_bn: False\n",
    "  edge_encoder: True\n",
    "  edge_encoder_name: DummyEdge\n",
    "  edge_encoder_bn: False\n",
    "posenc_RWSE:\n",
    "  enable: True\n",
    "  kernel:\n",
    "    times_func: range(1,21)\n",
    "  model: Linear\n",
    "  dim_pe: 28\n",
    "  raw_norm_type: BatchNorm\n",
    "train:\n",
    "  mode: custom\n",
    "  batch_size: {batch_size}\n",
    "  eval_period: 1\n",
    "  ckpt_period: 100\n",
    "model:\n",
    "  type: GPSModel\n",
    "  loss_fun: {loss_fn}\n",
    "  graph_pooling: mean\n",
    "  edge_decoding: dot\n",
    "gt:\n",
    "  layer_type: CustomGatedGCN+Transformer\n",
    "  layers: {num_layers}\n",
    "  n_heads: {num_heads}\n",
    "  dim_hidden: {dim_hidden}  # `gt.dim_hidden` must match `gnn.dim_inner`\n",
    "  dropout: 0.0\n",
    "  attn_dropout: {attn_dropout}\n",
    "  layer_norm: False\n",
    "  batch_norm: True\n",
    "gnn:\n",
    "  head: default\n",
    "  layers_pre_mp: 0\n",
    "  layers_post_mp: 3  # Not used when `gnn.head: san_graph`\n",
    "  dim_inner: {dim_hidden}  # `gt.dim_hidden` must match `gnn.dim_inner`\n",
    "  batchnorm: True\n",
    "  act: gelu\n",
    "  dropout: 0.0\n",
    "optim:\n",
    "  clip_grad_norm: True\n",
    "  optimizer: adamW\n",
    "  weight_decay: 1e-10\n",
    "  base_lr: 0.0005\n",
    "  max_epoch: 500\n",
    "  scheduler: reduce_on_plateau\n",
    "  reduce_factor: 0.5\n",
    "  schedule_patience: 15\n",
    "  min_lr: 1e-5\n",
    "  early_stopping_patience: 30\n",
    "share:\n",
    "  dim_out: {num_classes}\n",
    "seed: {seed}'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make sure to add your correct paths!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For QM9 and DOCKSTRING you need to specify a target in the template above and in the generation code below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ds in ['IMDB-MULTI']:\n",
    "    batch_size = 32\n",
    "    has_edges = True\n",
    "\n",
    "    loss_fn = 'cross_entropy'\n",
    "    metric_best = 'mcc'\n",
    "\n",
    "    task_type = 'classification_multi'\n",
    "\n",
    "    num_classes = NUM_CLASSES[ds]\n",
    "\n",
    "    for seed in [0]:\n",
    "        for dim_hidden in [256]:\n",
    "            for num_layers in [6]:\n",
    "                for num_heads in [16]:\n",
    "                    for attn_dropout in [0.1, 0.5]:\n",
    "                        CONFIG_PATH = '<YOUR_PATH>/training_configs_IMDB_MULTI'\n",
    "                        SCRIPT_PATH = '<YOUR_PATH>/training_scripts_IMDB_MULTI'\n",
    "\n",
    "                        Path(CONFIG_PATH).mkdir(exist_ok=True, parents=True)\n",
    "                        Path(SCRIPT_PATH).mkdir(exist_ok=True, parents=True)\n",
    "                    \n",
    "                        dl_dir = f'<DL_PATH>/{ds}'\n",
    "                        out_dir = f'<OUT_PATH>/{ds}/{seed}/{dim_hidden}/{num_layers}/{num_heads}/{attn_dropout}'\n",
    "                        wandb_proj_name = '<WANDB_PROJ>'\n",
    "\n",
    "                        conf = template_config.format(\n",
    "                            ds=ds, seed=seed, out_dir=out_dir, loss_fn=loss_fn, dim_hidden=dim_hidden, num_layers=num_layers, num_heads=num_heads,\n",
    "                            metric_best=metric_best, task_type=task_type, ds_download_dir=dl_dir, wandb_proj_name=wandb_proj_name, batch_size=batch_size,\n",
    "                            has_edges=has_edges, num_classes=num_classes, attn_dropout=attn_dropout\n",
    "                        )\n",
    "                        with open(os.path.join(CONFIG_PATH, f'{ds}_{seed}_{dim_hidden}_{num_layers}_{num_heads}_{attn_dropout}.yaml'), 'w') as f:\n",
    "                            f.write(conf)\n",
    "\n",
    "                        script = f'python <YOUR_PATH>/graphgps_graph/main.py --cfg {CONFIG_PATH}/{ds}_{seed}_{dim_hidden}_{num_layers}_{num_heads}_{attn_dropout}.yaml'\n",
    "                        with open(f'{SCRIPT_PATH}/{ds}_{seed}_{dim_hidden}_{num_layers}_{num_heads}_{attn_dropout}.sh', 'w') as f:\n",
    "                            f.write(script)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
